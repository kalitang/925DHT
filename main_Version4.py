import requests
import yaml
import random
from concurrent.futures import ThreadPoolExecutor

# å¯è¡¥å……æ›´å¤šå…è´¹clashè®¢é˜…æº
SUB_LINKS = [
    "https://raw.githubusercontent.com/ACL4SSR/ACL4SSR/master/Clash/ClashPremium.yaml",
    "https://raw.githubusercontent.com/learnhard-cn/free_proxy_ss/main/clash.yaml",
    "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml"
]

OUTPUT_FILE = "subscription.yaml"
FINAL_NODE_COUNT = 20

RULE_TEMPLATE = '''
port: 7890
socks-port: 7891
allow-lan: true
mode: rule
log-level: info
external-controller: :9090
proxies: []
proxy-groups:
  - name: ğŸš€ èŠ‚ç‚¹é€‰æ‹©
    type: select
    proxies: []
  - name: â™»ï¸ è‡ªåŠ¨é€‰æ‹©
    type: url-test
    proxies: []
    url: http://www.gstatic.com/generate_204
    interval: 300
  - name: ğŸ¥ æµåª’ä½“è§£é”
    type: select
    proxies:
      - ğŸš€ èŠ‚ç‚¹é€‰æ‹©
      - DIRECT
  - name: ğŸ è‹¹æœæœåŠ¡
    type: select
    proxies:
      - DIRECT
      - ğŸš€ èŠ‚ç‚¹é€‰æ‹©
  - name: ğŸ›‘ å¹¿å‘Šæ‹¦æˆª
    type: select
    proxies:
      - REJECT
      - DIRECT
      - ğŸš€ èŠ‚ç‚¹é€‰æ‹©
rules:
  - DOMAIN-SUFFIX,local,DIRECT
  - DOMAIN-KEYWORD,google,ğŸš€ èŠ‚ç‚¹é€‰æ‹©
  - DOMAIN-KEYWORD,facebook,ğŸš€ èŠ‚ç‚¹é€‰æ‹©
  - DOMAIN-KEYWORD,netflix,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,openai,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,telegram,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,youtube,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,twitter,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,instagram,ğŸ¥ æµåª’ä½“è§£é”
  - DOMAIN-KEYWORD,github,ğŸš€ èŠ‚ç‚¹é€‰æ‹©
  - GEOIP,CN,DIRECT
  - MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©
'''

def fetch_yaml(url):
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        data = yaml.safe_load(resp.text)
        if isinstance(data, dict) and 'proxies' in data:
            return data['proxies']
    except Exception as e:
        print(f"Failed fetch {url}: {e}")
    return []

def test_proxy(proxy):
    # è™šæ‹Ÿæµ‹é€Ÿï¼ˆå¦‚éœ€çœŸå®æµ‹é€Ÿå¯ç”¨ clash/api æˆ– ping ç­‰æ–¹å¼æ‹“å±•ï¼‰
    delay = random.randint(30, 800)
    return delay

def main():
    all_nodes = []
    for link in SUB_LINKS:
        proxies = fetch_yaml(link)
        if proxies:
            all_nodes.extend(proxies)
        if len(all_nodes) >= 50:
            break

    # å»é‡
    seen = set()
    uniq_nodes = []
    for p in all_nodes:
        key = f"{p.get('name','')}-{p.get('server','')}-{p.get('port','')}"
        if key not in seen:
            uniq_nodes.append(p)
            seen.add(key)

    # ç®€å•æµ‹é€Ÿï¼ˆæŒ‰å»¶è¿Ÿå‡åºå–å‰20ï¼‰
    with ThreadPoolExecutor(max_workers=10) as pool:
        delays = list(pool.map(test_proxy, uniq_nodes))
    nodes_with_delay = sorted(zip(uniq_nodes, delays), key=lambda x: x[1])
    fast_nodes = [n for n, d in nodes_with_delay[:FINAL_NODE_COUNT]]

    # ç”Ÿæˆ yaml
    tpl = yaml.safe_load(RULE_TEMPLATE)
    tpl['proxies'] = fast_nodes
    proxy_names = [x['name'] for x in fast_nodes]
    for g in tpl['proxy-groups']:
        if g['name'] in ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©']:
            g['proxies'] = proxy_names
        elif g['name'] == 'ğŸ¥ æµåª’ä½“è§£é”':
            g['proxies'] = ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©'] + proxy_names

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        yaml.dump(tpl, f, allow_unicode=True)

if __name__ == '__main__':
    main()